{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers psutil --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:33:21.478052Z","iopub.execute_input":"2025-10-05T12:33:21.478392Z","iopub.status.idle":"2025-10-05T12:33:24.800521Z","shell.execute_reply.started":"2025-10-05T12:33:21.478330Z","shell.execute_reply":"2025-10-05T12:33:24.799493Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Imports & Setup","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom datasets import load_dataset\nfrom transformers import GPT2TokenizerFast\nimport psutil, os\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"‚úÖ Using device: {device.upper()}\")\n\n# Print system resource info\nram_gb = round(psutil.virtual_memory().total / (1024**3), 2)\nprint(f\"üíæ System RAM: {ram_gb} GB\")\nif device == \"cuda\":\n    gpu_mem = round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2)\n    print(f\"‚öôÔ∏è GPU Memory: {gpu_mem} GB ({torch.cuda.get_device_name(0)})\")\n\ntorch.manual_seed(1337)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:33:29.654051Z","iopub.execute_input":"2025-10-05T12:33:29.654380Z","iopub.status.idle":"2025-10-05T12:33:36.193504Z","shell.execute_reply.started":"2025-10-05T12:33:29.654319Z","shell.execute_reply":"2025-10-05T12:33:36.192777Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using device: CUDA\nüíæ System RAM: 31.35 GB\n‚öôÔ∏è GPU Memory: 15.83 GB (Tesla T4)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c2049d33550>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"code","source":"batch_size = 32\nblock_size = 128\nmax_iters = 50000      \neval_interval = 1000\nlearning_rate = 3e-4\neval_iters = 100\nn_embd = 256\nn_head = 8\nn_layer = 6             \ndropout = 0.1\n\nprint(\"üìò Training Configuration:\")\nprint(f\"batch_size={batch_size}, block_size={block_size}, n_layer={n_layer}, n_head={n_head}, n_embd={n_embd}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:33:51.988886Z","iopub.execute_input":"2025-10-05T12:33:51.989361Z","iopub.status.idle":"2025-10-05T12:33:51.994800Z","shell.execute_reply.started":"2025-10-05T12:33:51.989320Z","shell.execute_reply":"2025-10-05T12:33:51.994017Z"}},"outputs":[{"name":"stdout","text":"üìò Training Configuration:\nbatch_size=32, block_size=128, n_layer=6, n_head=8, n_embd=256\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Load WikiText Dataset","metadata":{}},{"cell_type":"code","source":"print(\"üì• Loading WikiText-103 (50%)...\")\nds = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train[:50%]\")\nprint(f\"‚úÖ Loaded {len(ds)} documents\")\n\n# Preview a few samples\nfor i in range(2):\n    print(f\"\\nüìù Sample {i+1}:\\n{ds[i]['text'][:300]}...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:33:59.729614Z","iopub.execute_input":"2025-10-05T12:33:59.729873Z","iopub.status.idle":"2025-10-05T12:34:09.123080Z","shell.execute_reply.started":"2025-10-05T12:33:59.729853Z","shell.execute_reply":"2025-10-05T12:34:09.122367Z"}},"outputs":[{"name":"stdout","text":"üì• Loading WikiText-103 (50%)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30e897d23440442792a4edeb40149b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/test-00000-of-00001.(‚Ä¶):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"831941ab96824e2bbf1e70afc7bf19af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/train-00000-of-00002(‚Ä¶):   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2ddea30240d4ffcbbf21b5921d81693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/train-00001-of-00002(‚Ä¶):   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd29f385c58d4307ae7fdf23fe2cd688"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/validation-00000-of-(‚Ä¶):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ea1c13be79c4903a450ed368e8a5806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b36ae44da291431da97a76b9006ffc7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"453585b168d6474f88fba2d9e355e4c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71ee8c6a5f6f4d42a5de8f82d7fc8d7d"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Loaded 900675 documents\n\nüìù Sample 1:\n...\n\nüìù Sample 2:\n = Valkyria Chronicles III = \n...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Tokenizer and Encoding","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"üî§ Encoding dataset using GPT-2 BPE tokenizer...\")\nsample_text = \"Transformers are powerful models for language understanding.\"\nencoded = tokenizer.encode(sample_text)\ndecoded = tokenizer.decode(encoded)\nprint(f\"üß© Sample text:\\n{sample_text}\")\nprint(f\"‚û°Ô∏è Encoded tokens: {encoded}\")\nprint(f\"‚Ü©Ô∏è Decoded back: {decoded}\")\n\n# Tokenize entire dataset\ntokens = []\nfor item in ds:\n    if item.get(\"text\"):\n        tokens.extend(tokenizer.encode(item[\"text\"]))\ndata = torch.tensor(tokens, dtype=torch.long)\n\nprint(f\"\\nüìà Total tokens: {len(data):,}\")\nvocab_size = tokenizer.vocab_size\nprint(f\"üìö Vocabulary size: {vocab_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:34:24.071945Z","iopub.execute_input":"2025-10-05T12:34:24.072534Z","iopub.status.idle":"2025-10-05T12:37:59.993268Z","shell.execute_reply.started":"2025-10-05T12:34:24.072504Z","shell.execute_reply":"2025-10-05T12:37:59.992575Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61024df84f1e454d881175638857ddf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73599458053d4b81b2ddcbea2b37e483"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38be82347c7640ea8064827d58c51d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f978f3e9421643e38d5bee4466222835"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f336be299344819370679bbe1d571d"}},"metadata":{}},{"name":"stdout","text":"üî§ Encoding dataset using GPT-2 BPE tokenizer...\nüß© Sample text:\nTransformers are powerful models for language understanding.\n‚û°Ô∏è Encoded tokens: [41762, 364, 389, 3665, 4981, 329, 3303, 4547, 13]\n‚Ü©Ô∏è Decoded back: Transformers are powerful models for language understanding.\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"\nüìà Total tokens: 58,984,516\nüìö Vocabulary size: 50257\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Data Split & Batch Sampling","metadata":{}},{"cell_type":"code","source":"n = int(0.9 * len(data))\ntrain_data, val_data = data[:n], data[n:]\n\nprint(f\"üß† Train data tokens: {len(train_data):,}\")\nprint(f\"üß™ Validation tokens: {len(val_data):,}\")\n\ndef get_batch(split):\n    data_local = train_data if split == \"train\" else val_data\n    ix = torch.randint(len(data_local) - block_size, (batch_size,))\n    x = torch.stack([data_local[i:i+block_size] for i in ix])\n    y = torch.stack([data_local[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\nxb, yb = get_batch(\"train\")\nprint(f\"‚úÖ Batch X shape: {xb.shape}, Y shape: {yb.shape}\")\nprint(f\"üî¢ Example token IDs:\\n{xb[0][:20].tolist()}\")\nprint(f\"üó£ Decoded snippet:\\n{tokenizer.decode(xb[0][:50].tolist())}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:37:59.994454Z","iopub.execute_input":"2025-10-05T12:37:59.994934Z","iopub.status.idle":"2025-10-05T12:38:00.188811Z","shell.execute_reply.started":"2025-10-05T12:37:59.994915Z","shell.execute_reply":"2025-10-05T12:38:00.188015Z"}},"outputs":[{"name":"stdout","text":"üß† Train data tokens: 53,086,064\nüß™ Validation tokens: 5,898,452\n‚úÖ Batch X shape: torch.Size([32, 128]), Y shape: torch.Size([32, 128])\nüî¢ Example token IDs:\n[262, 14555, 43469, 2168, 837, 2716, 416, 3966, 89, 5799, 739, 262, 3670, 1012, 1236, 324, 837, 373, 2716, 287]\nüó£ Decoded snippet:\n the earliest anthology series , released by Ohzora under the title Clannad , was released in June 2004 under their Twin Heart Comics imprint . Volumes for this series continued to be released until April 2005 with the fifth volume . The second anthology was\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Transformer Components","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        return self.dropout(self.proj(out))\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n    def forward(self, x): return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:38:33.389860Z","iopub.execute_input":"2025-10-05T12:38:33.390537Z","iopub.status.idle":"2025-10-05T12:38:33.400064Z","shell.execute_reply.started":"2025-10-05T12:38:33.390510Z","shell.execute_reply":"2025-10-05T12:38:33.399131Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# GPT Architecture","metadata":{}},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens, temperature=1.0):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n\nmodel = GPT().to(device)\nparam_count = sum(p.numel() for p in model.parameters())\nparam_size_gb = (param_count * 4) / (1024**3)  # assuming float32\nprint(f\"üßÆ Model parameters: {param_count:,} ({param_count/1e6:.2f}M)\")\nprint(f\"üíæ Model parameter size ‚âà {param_size_gb:.3f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:38:44.114415Z","iopub.execute_input":"2025-10-05T12:38:44.114936Z","iopub.status.idle":"2025-10-05T12:38:44.437254Z","shell.execute_reply.started":"2025-10-05T12:38:44.114909Z","shell.execute_reply":"2025-10-05T12:38:44.436517Z"}},"outputs":[{"name":"stdout","text":"üßÆ Model parameters: 30,549,073 (30.55M)\nüíæ Model parameter size ‚âà 0.114 GB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n@torch.no_grad()\ndef estimate_loss():\n    model.eval()\n    out = {}\n    for split in [\"train\", \"val\"]:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            xb, yb = get_batch(split)\n            _, loss = model(xb, yb)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nprint(\"üöÄ Starting training...\")\nfor it in range(max_iters):\n    if it % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {it:5d}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n        torch.save(model.state_dict(), \"mini_gpt.pt\")\n\n    xb, yb = get_batch(\"train\")\n    _, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:38:58.441549Z","iopub.execute_input":"2025-10-05T12:38:58.442036Z","iopub.status.idle":"2025-10-05T15:43:13.888172Z","shell.execute_reply.started":"2025-10-05T12:38:58.442013Z","shell.execute_reply":"2025-10-05T15:43:13.887481Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting training...\nstep     0: train 11.0028, val 11.0028\nstep  1000: train 6.1067, val 6.1233\nstep  2000: train 5.6809, val 5.7316\nstep  3000: train 5.4047, val 5.4497\nstep  4000: train 5.2148, val 5.3042\nstep  5000: train 5.0828, val 5.1641\nstep  6000: train 4.9728, val 5.0710\nstep  7000: train 4.9058, val 4.9813\nstep  8000: train 4.8153, val 4.9380\nstep  9000: train 4.7651, val 4.8704\nstep 10000: train 4.7241, val 4.8243\nstep 11000: train 4.6535, val 4.7817\nstep 12000: train 4.6079, val 4.7101\nstep 13000: train 4.5650, val 4.6897\nstep 14000: train 4.5315, val 4.6706\nstep 15000: train 4.4776, val 4.6525\nstep 16000: train 4.4754, val 4.6061\nstep 17000: train 4.4306, val 4.5953\nstep 18000: train 4.4210, val 4.5778\nstep 19000: train 4.3762, val 4.5472\nstep 20000: train 4.3597, val 4.5232\nstep 21000: train 4.3347, val 4.5116\nstep 22000: train 4.3074, val 4.4819\nstep 23000: train 4.3079, val 4.4741\nstep 24000: train 4.2817, val 4.4695\nstep 25000: train 4.2805, val 4.4371\nstep 26000: train 4.2520, val 4.4475\nstep 27000: train 4.2433, val 4.4119\nstep 28000: train 4.2269, val 4.4272\nstep 29000: train 4.1962, val 4.3970\nstep 30000: train 4.2064, val 4.3978\nstep 31000: train 4.1833, val 4.3886\nstep 32000: train 4.1572, val 4.3827\nstep 33000: train 4.1614, val 4.3635\nstep 34000: train 4.1525, val 4.3475\nstep 35000: train 4.1275, val 4.3289\nstep 36000: train 4.1372, val 4.3241\nstep 37000: train 4.1209, val 4.3120\nstep 38000: train 4.1124, val 4.3264\nstep 39000: train 4.0854, val 4.3173\nstep 40000: train 4.1001, val 4.3023\nstep 41000: train 4.0816, val 4.3029\nstep 42000: train 4.0699, val 4.2920\nstep 43000: train 4.0716, val 4.2598\nstep 44000: train 4.0392, val 4.2739\nstep 45000: train 4.0335, val 4.2746\nstep 46000: train 4.0382, val 4.2502\nstep 47000: train 4.0234, val 4.2456\nstep 48000: train 4.0351, val 4.2387\nstep 49000: train 4.0036, val 4.2283\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"prompt = (\n    \"Artificial intelligence is changing the world in profound ways. \"\n    \"From self-driving cars to healthcare diagnostics, AI is enabling \"\n    \"innovations that were once considered science fiction. Experts believe \"\n    \"that the next decade will see AI integrated into nearly every aspect \"\n    \"of daily life, transforming industries, education, and human interactions.\"\n)\n\ninput_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\nout_idx = model.generate(input_ids, max_new_tokens=50, temperature=0.8)[0].tolist()\nprint(\"\\n----- GENERATED TEXT -----\\n\")\nprint(tokenizer.decode(out_idx))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T16:15:23.226937Z","iopub.execute_input":"2025-10-05T16:15:23.227198Z","iopub.status.idle":"2025-10-05T16:15:24.378078Z","shell.execute_reply.started":"2025-10-05T16:15:23.227178Z","shell.execute_reply":"2025-10-05T16:15:24.377469Z"}},"outputs":[{"name":"stdout","text":"\n----- GENERATED TEXT -----\n\nArtificial intelligence is changing the world in profound ways. From self-driving cars to healthcare diagnostics, AI is enabling innovations that were once considered science fiction. Experts believe that the next decade will see AI integrated into nearly every aspect of daily life, transforming industries, education, and human interactions. in relatively recent decades . The 1988 media article Telescope that the 2007 academic survey of the White Motorism Project was \" pure \" , that it reduces class the future of the American community is \" so far more attitude than any state of the ongoing fighting class\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\ntorch.save(model.state_dict(), \"mini_gpt.pt\")\n\n# Later, to load:\nmodel = GPT().to(device)  \nmodel.load_state_dict(torch.load(\"mini_gpt.pt\", map_location=device))\nmodel.eval()  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T16:17:13.739877Z","iopub.execute_input":"2025-10-05T16:17:13.740585Z","iopub.status.idle":"2025-10-05T16:17:14.629176Z","shell.execute_reply.started":"2025-10-05T16:17:13.740558Z","shell.execute_reply":"2025-10-05T16:17:14.628260Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"GPT(\n  (token_embedding_table): Embedding(50257, 256)\n  (position_embedding_table): Embedding(128, 256)\n  (blocks): Sequential(\n    (0): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-7): 8 x Head(\n            (key): Linear(in_features=256, out_features=32, bias=False)\n            (query): Linear(in_features=256, out_features=32, bias=False)\n            (value): Linear(in_features=256, out_features=32, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-7): 8 x Head(\n            (key): Linear(in_features=256, out_features=32, bias=False)\n            (query): Linear(in_features=256, out_features=32, bias=False)\n            (value): Linear(in_features=256, out_features=32, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-7): 8 x Head(\n            (key): Linear(in_features=256, out_features=32, bias=False)\n            (query): Linear(in_features=256, out_features=32, bias=False)\n            (value): Linear(in_features=256, out_features=32, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-7): 8 x Head(\n            (key): Linear(in_features=256, out_features=32, bias=False)\n            (query): Linear(in_features=256, out_features=32, bias=False)\n            (value): Linear(in_features=256, out_features=32, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-7): 8 x Head(\n            (key): Linear(in_features=256, out_features=32, bias=False)\n            (query): Linear(in_features=256, out_features=32, bias=False)\n            (value): Linear(in_features=256, out_features=32, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-7): 8 x Head(\n            (key): Linear(in_features=256, out_features=32, bias=False)\n            (query): Linear(in_features=256, out_features=32, bias=False)\n            (value): Linear(in_features=256, out_features=32, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  (lm_head): Linear(in_features=256, out_features=50257, bias=True)\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}