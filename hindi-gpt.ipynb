{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport math\nimport csv\nimport random\nfrom pathlib import Path\nfrom datetime import datetime\n\nimport psutil\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tokenizers import ByteLevelBPETokenizer\nfrom transformers import GPT2TokenizerFast\nfrom datasets import load_dataset\n\n# -------------------- USER CONFIG --------------------\nOUT_DIR = Path(\"/kaggle/working/hindi-gpt\")\nCKPT_DIR = OUT_DIR / \"checkpoints\"\nTK_DIR = OUT_DIR / \"tokenizer\"\nLOG_DIR = OUT_DIR / \"logs\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nCKPT_DIR.mkdir(exist_ok=True)\nTK_DIR.mkdir(exist_ok=True)\nLOG_DIR.mkdir(exist_ok=True)\n\n# Model / tokenizer config\nBLOCK_SIZE = 128      # context size\nD_MODEL = 512         # embedding / model dim\nN_HEAD = 8\nN_LAYER = 8\nDROPOUT = 0.1\nVOCAB_SIZE = 32000    \n\n# Training config\nTOTAL_STEPS = 50000   \nBATCH_SIZE = 16       \nEVAL_INTERVAL = 2000\nEVAL_ITERS = 100\nLEARNING_RATE = 3e-4\nSAVE_EVERY = 2000\nSEED = 1337\n\n# Data selection\nTARGET_BYTES = 1 * 1024**3   # ~1 GB raw text\nMAX_DOCS = None              # cap docs (None => no cap)\n\n# Misc\nUSE_GRAD_ACCUM = False\nGRAD_ACCUM_STEPS = 2\nGENERATE_TOKENS = 200\nINFERENCE_PROMPT = \"भारत की तकनीकी प्रगति के बारे में विस्तृत जानकारी दीजिए और बताइए कि आने वाले वर्षों में भारत किस दिशा में आगे बढ़ेगा।\"\n\n# reproducibility\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\n# device info\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nn_gpus = torch.cuda.device_count()\nprint(f\"Device: {device}, GPUs: {n_gpus}\")\nif device == \"cuda\":\n    for i in range(n_gpus):\n        print(\" GPU\", i, \":\", torch.cuda.get_device_name(i))\nprint(\"RAM (GB):\", psutil.virtual_memory().total / 1e9)\n\n# -------------------- LOAD DATASET (nisram) & extract ~1GB --------------------\nprint(\"Loading nis12ram/nisram-hindi-text-0.0 ...\")\nds = load_dataset(\"nis12ram/nisram-hindi-text-0.0\", split=\"train\")\n\nall_texts = ds[\"text\"]\nprint(\"Total rows in dataset:\", len(all_texts))\n\nindices = list(range(len(all_texts)))\nrandom.shuffle(indices)\n\ntexts = []\nacc_bytes = 0\ncount = 0\nfor i in indices:\n    if MAX_DOCS is not None and count >= MAX_DOCS:\n        break\n    t = all_texts[i]\n    if not t:\n        continue\n    b = t.encode(\"utf-8\")\n    acc_bytes += len(b)\n    texts.append(t)\n    count += 1\n    if count % 20000 == 0:\n        print(f\"Collected {count:,} docs, {acc_bytes/1e9:.3f} GB\")\n    if acc_bytes >= TARGET_BYTES:\n        break\n\nprint(f\"Selected {len(texts):,} documents, approx {acc_bytes/1e9:.3f} GB\")\n\nraw_text_file = OUT_DIR / \"nisram_hi_raw.txt\"\nwith open(raw_text_file, \"w\", encoding=\"utf-8\") as f:\n    for t in texts:\n        line = t.replace(\"\\r\\n\", \"\\n\").strip()\n        if line == \"\":\n            continue\n        f.write(line + \"\\n\")\nprint(\"Saved raw text file:\", raw_text_file, \"size GB:\", raw_text_file.stat().st_size / 1e9)\n\n# -------------------- TRAIN BYTE-LEVEL BPE TOKENIZER --------------------\nprint(\"Training ByteLevel BPE tokenizer (vocab_size = {}) ...\".format(VOCAB_SIZE))\nbpe = ByteLevelBPETokenizer()\nbpe.train(files=[str(raw_text_file)], vocab_size=VOCAB_SIZE, min_frequency=2,\n          special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\nbpe.save_model(str(TK_DIR))\nprint(\"Tokenizer model files saved to:\", TK_DIR)\n\n# wrap as HuggingFace fast tokenizer\ntokenizer = GPT2TokenizerFast.from_pretrained(str(TK_DIR))\ntokenizer.add_special_tokens({\n    \"bos_token\": \"<s>\",\n    \"eos_token\": \"</s>\",\n    \"unk_token\": \"<unk>\",\n    \"pad_token\": \"<pad>\",\n})\nprint(\"Tokenizer vocab size reported:\", tokenizer.vocab_size)\n\n# -------------------- TOKENIZE & BUILD 1D TOKEN STREAM --------------------\nprint(\"Tokenizing texts and building token stream ...\")\nall_ids = []\nbatch_tokenize = 500\nfor i in range(0, len(texts), batch_tokenize):\n    batch = texts[i:i+batch_tokenize]\n    enc = tokenizer(batch, add_special_tokens=False)\n    for seq in enc[\"input_ids\"]:\n        all_ids.extend(seq + [tokenizer.eos_token_id])\n    if (i // batch_tokenize) % 20 == 0:\n        print(f\"Tokenized up to doc {i+len(batch):,}, tokens so far: {len(all_ids):,}\")\n\ndata = torch.tensor(all_ids, dtype=torch.long)\nprint(\"Total tokens in dataset:\", len(data))\n\n# -------------------- TRAIN / VAL SPLIT --------------------\nn = int(0.9 * len(data))\ntrain_data = data[:n].to(device)\nval_data = data[n:].to(device)\nprint(\"Train tokens:\", len(train_data), \"Val tokens:\", len(val_data))\n\n# -------------------- BATCH / GET_BATCH --------------------\ndef get_batch(split):\n    data_local = train_data if split == \"train\" else val_data\n    max_start = len(data_local) - BLOCK_SIZE - 1\n    ix = torch.randint(0, max_start, (BATCH_SIZE,))\n    x = torch.stack([data_local[i:i+BLOCK_SIZE] for i in ix])\n    y = torch.stack([data_local[i+1:i+BLOCK_SIZE+1] for i in ix])\n    return x, y\n\n# -------------------- MODEL DEFINITION (GPT-style) --------------------\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(D_MODEL, head_size, bias=False)\n        self.query = nn.Linear(D_MODEL, head_size, bias=False)\n        self.value = nn.Linear(D_MODEL, head_size, bias=False)\n        self.register_buffer(\"tril\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n        self.dropout = nn.Dropout(DROPOUT)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (q.size(-1) ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        return wei @ v\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(D_MODEL, D_MODEL)\n        self.dropout = nn.Dropout(DROPOUT)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        return self.dropout(self.proj(out))\n\nclass FeedForward(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(D_MODEL, 4 * D_MODEL),\n            nn.GELU(),\n            nn.Linear(4 * D_MODEL, D_MODEL),\n            nn.Dropout(DROPOUT),\n        )\n    def forward(self, x): return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self):\n        super().__init__()\n        head_size = D_MODEL // N_HEAD\n        self.sa = MultiHeadAttention(N_HEAD, head_size)\n        self.ffwd = FeedForward()\n        self.ln1 = nn.LayerNorm(D_MODEL)\n        self.ln2 = nn.LayerNorm(D_MODEL)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(tokenizer.vocab_size, D_MODEL)\n        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, D_MODEL)\n        self.blocks = nn.Sequential(*[Block() for _ in range(N_LAYER)])\n        self.ln_f = nn.LayerNorm(D_MODEL)\n        self.lm_head = nn.Linear(D_MODEL, tokenizer.vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        loss = None\n        if targets is not None:\n            # ensure reduction='mean' so per-GPU outputs can be averaged safely\n            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T), reduction='mean')\n        return logits, loss\n\n# -------------------- INSTANTIATE MODEL & OPTIMIZER --------------------\nmodel = GPT().to(device)\nparam_count = sum(p.numel() for p in model.parameters())\nprint(f\"Model parameters: {param_count/1e6:.2f}M\")\n\n# DataParallel (simple multi-GPU)\nif device == \"cuda\" and n_gpus > 1:\n    model = nn.DataParallel(model)\n    print(\"Wrapped model in DataParallel for\", n_gpus, \"GPUs\")\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\nscaler = torch.amp.GradScaler(enabled=(device==\"cuda\"))\n\n# -------------------- EVAL / METRICS --------------------\n@torch.no_grad()\ndef estimate_loss():\n    model.eval()\n    out = {}\n    for split in [\"train\", \"val\"]:\n        losses = []\n        for _ in range(EVAL_ITERS):\n            xb, yb = get_batch(split)\n            xb = xb.to(device); yb = yb.to(device)\n            with torch.amp.autocast(device_type='cuda' if device=='cuda' else 'cpu', enabled=(device==\"cuda\")):\n                _, loss = model(xb, yb)\n            # reduce before reading (handles DataParallel returning per-device tensors)\n            losses.append(loss.mean().item())\n        out[split] = float(np.mean(losses))\n    model.train()\n    return out\n\ndef ppl_from_loss(loss_val):\n    return math.exp(loss_val) if loss_val < 50 else float(\"inf\")\n\n# -------------------- GENERATION (greedy) --------------------\n@torch.no_grad()\ndef generate_text(prompt, max_new_tokens=200):\n    model.eval()\n    tokens = tokenizer.encode(prompt)\n    idx = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n    for _ in range(max_new_tokens):\n        if idx.size(1) > BLOCK_SIZE:\n            idx_cond = idx[:, -BLOCK_SIZE:]\n        else:\n            idx_cond = idx\n        with torch.amp.autocast(device_type='cuda' if device=='cuda' else 'cpu', enabled=(device==\"cuda\")):\n            logits, _ = model(idx_cond)\n        logits = logits[:, -1, :]\n        next_id = torch.argmax(F.softmax(logits, dim=-1), dim=-1).unsqueeze(0)\n        idx = torch.cat((idx, next_id), dim=1)\n    text = tokenizer.decode(idx.squeeze().tolist(), skip_special_tokens=True)\n    model.train()\n    return text\n\n# -------------------- LOGGING SETUP --------------------\nlog_csv = LOG_DIR / \"train_log.csv\"\nwith open(log_csv, \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"step\",\"train_loss_iter\",\"eval_train_loss\",\"eval_val_loss\",\"eval_train_ppl\",\"eval_val_ppl\",\"step_time_s\",\"save_time_s\",\"timestamp\"])\n\nloss_history = []\neval_history = []\nperp_history = []\ntimings = []\n\n# -------------------- TRAIN LOOP --------------------\nprint(\"Starting training for\", TOTAL_STEPS, \"steps ...\")\nglobal_start = time.time()\nstep = 0\n\n# initial eval\ninit = estimate_loss()\nprint(f\"Initial eval — Train {init['train']:.4f}, Val {init['val']:.4f}\")\n\nwhile step < TOTAL_STEPS:\n    step += 1\n    t0 = time.time()\n\n    xb, yb = get_batch(\"train\")\n    xb, yb = xb.to(device), yb.to(device)\n\n    # forward/backward with AMP\n    with torch.amp.autocast(device_type='cuda' if device=='cuda' else 'cpu', enabled=(device==\"cuda\")):\n        logits, loss = model(xb, yb)\n\n    # reduce across devices (in case DataParallel returns vector-like loss)\n    loss = loss.mean()\n\n    # gradient scaling / backward\n    loss_to_backprop = loss / (GRAD_ACCUM_STEPS if USE_GRAD_ACCUM else 1)\n    scaler.scale(loss_to_backprop).backward()\n\n    if (not USE_GRAD_ACCUM) or (step % GRAD_ACCUM_STEPS == 0):\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad(set_to_none=True)\n\n    step_time = time.time() - t0\n    iter_train_loss = float(loss.item())\n    loss_history.append((step, iter_train_loss))\n\n    save_time_s = \"\"\n    # eval and checkpointing\n    if step % EVAL_INTERVAL == 0 or step == TOTAL_STEPS:\n        t_eval_start = time.time()\n        metrics = estimate_loss()\n        t_eval = time.time() - t_eval_start\n        train_loss_eval = metrics[\"train\"]\n        val_loss_eval = metrics[\"val\"]\n        train_ppl = ppl_from_loss(train_loss_eval)\n        val_ppl = ppl_from_loss(val_loss_eval)\n        print(f\"[Step {step}] iter_loss {iter_train_loss:.4f} | Eval Train {train_loss_eval:.4f} Val {val_loss_eval:.4f} | ValPPL {val_ppl:.2f} | eval_time {t_eval:.1f}s\")\n        eval_history.append((step, train_loss_eval, val_loss_eval))\n        perp_history.append((step, val_ppl))\n\n    if step % SAVE_EVERY == 0 or step == TOTAL_STEPS:\n        t_save_start = time.time()\n        model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n        torch.save(model_to_save.state_dict(), CKPT_DIR / f\"gpt2_hindi_step{step}.pt\")\n        tokenizer.save_pretrained(str(TK_DIR))\n        save_time_s = time.time() - t_save_start\n        print(f\"Saved checkpoint step {step} (save_time {save_time_s:.2f}s)\")\n\n    # append log row\n    with open(log_csv, \"a\", newline=\"\") as f:\n        writer = csv.writer(f)\n        row = [\n            step,\n            iter_train_loss,\n            train_loss_eval if (\"train_loss_eval\" in locals() or \"train_loss_eval\" in globals()) else \"\",\n            val_loss_eval if (\"val_loss_eval\" in locals() or \"val_loss_eval\" in globals()) else \"\",\n            ppl_from_loss(train_loss_eval) if (\"train_loss_eval\" in locals() or \"train_loss_eval\" in globals()) else \"\",\n            val_ppl if (\"val_ppl\" in locals() or \"val_ppl\" in globals()) else \"\",\n            round(step_time, 6),\n            save_time_s,\n            datetime.utcnow().isoformat()\n        ]\n        writer.writerow(row)\n\n    timings.append({\"step\": step, \"step_time_s\": step_time, \"loss\": iter_train_loss, \"save_time_s\": save_time_s})\n\n# END training\ntotal_time = time.time() - global_start\nprint(\"Training finished. Total time (s):\", total_time)\n\n# final save\nt_final_save_start = time.time()\nmodel_to_save = model.module if isinstance(model, nn.DataParallel) else model\ntorch.save(model_to_save.state_dict(), CKPT_DIR / \"gpt2_hindi_final.pt\")\ntokenizer.save_pretrained(str(TK_DIR))\nt_final_save = time.time() - t_final_save_start\nprint(\"Final model saved (s):\", t_final_save)\n\n# -------------------- INFERENCE TIMING --------------------\nprint(\"Generating sample for prompt (timing)...\")\nt_inf_start = time.time()\ngenerated = generate_text(INFERENCE_PROMPT, max_new_tokens=GENERATE_TOKENS)\nt_inf = time.time() - t_inf_start\nprint(\"Inference time (s) for {} tokens: {:.3f}s\".format(GENERATE_TOKENS, t_inf))\n\nwith open(OUT_DIR / \"sample_generation.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"PROMPT:\\n\" + INFERENCE_PROMPT + \"\\n\\n\")\n    f.write(\"GENERATED:\\n\" + generated + \"\\n\\n\")\n    f.write(f\"Inference time (s): {t_inf}\\n\")\n\n# -------------------- PLOTS --------------------\nsteps_arr = [s for s, l in loss_history]\nloss_arr = [l for s, l in loss_history]\neval_steps = [s for s, tr, va in eval_history]\neval_train_arr = [tr for s, tr, va in eval_history]\neval_val_arr = [va for s, tr, va in eval_history]\nperp_steps = [s for s, p in perp_history]\nperp_arr = [p for s, p in perp_history]\n\nif len(loss_arr) > 0:\n    plt.figure(figsize=(10,6))\n    plt.plot(steps_arr, loss_arr, label=\"iter train loss\", alpha=0.7)\n    if len(eval_steps) > 0:\n        plt.plot(eval_steps, eval_train_arr, label=\"eval train loss\", marker=\"o\")\n        plt.plot(eval_steps, eval_val_arr, label=\"eval val loss\", marker=\"o\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss vs Step\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(OUT_DIR / \"loss_curve.png\", bbox_inches=\"tight\")\n    plt.close()\n    print(\"Saved loss plot:\", OUT_DIR / \"loss_curve.png\")\n\nif len(perp_arr) > 0:\n    plt.figure(figsize=(10,6))\n    plt.plot(perp_steps, perp_arr, label=\"val perplexity\", marker=\"o\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Perplexity\")\n    plt.title(\"Perplexity vs Step\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(OUT_DIR / \"perplexity_curve.png\", bbox_inches=\"tight\")\n    plt.close()\n    print(\"Saved perplexity plot:\", OUT_DIR / \"perplexity_curve.png\")\n\n# save timings CSV\ntimings_csv = OUT_DIR / \"timings.csv\"\nwith open(timings_csv, \"w\", newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=[\"step\",\"step_time_s\",\"loss\",\"save_time_s\"])\n    writer.writeheader()\n    for r in timings:\n        writer.writerow(r)\nprint(\"Saved timings CSV:\", timings_csv)\n\n# -------------------- SUMMARY --------------------\nsummary = {\n    \"total_steps\": TOTAL_STEPS,\n    \"batch_size\": BATCH_SIZE,\n    \"block_size\": BLOCK_SIZE,\n    \"d_model\": D_MODEL,\n    \"n_head\": N_HEAD,\n    \"n_layer\": N_LAYER,\n    \"vocab_size\": tokenizer.vocab_size,\n    \"final_checkpoint\": str(CKPT_DIR / \"gpt2_hindi_final.pt\"),\n    \"tokenizer_dir\": str(TK_DIR),\n    \"loss_plot\": str(OUT_DIR / \"loss_curve.png\"),\n    \"perplexity_plot\": str(OUT_DIR / \"perplexity_curve.png\"),\n    \"timings_csv\": str(timings_csv),\n    \"inference_time_s\": t_inf\n}\nprint(\"SUMMARY:\")\nfor k,v in summary.items():\n    print(f\"  {k}: {v}\")\n\nprint(\"All outputs saved under:\", OUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T12:30:14.510030Z","iopub.execute_input":"2025-11-18T12:30:14.510702Z","iopub.status.idle":"2025-11-18T16:06:34.270251Z","shell.execute_reply.started":"2025-11-18T12:30:14.510671Z","shell.execute_reply":"2025-11-18T16:06:34.269432Z"}},"outputs":[{"name":"stdout","text":"Device: cuda, GPUs: 2\n GPU 0 : Tesla T4\n GPU 1 : Tesla T4\nRAM (GB): 33.662328832\nLoading nis12ram/nisram-hindi-text-0.0 ...\nTotal rows in dataset: 601628\nCollected 20,000 docs, 0.091 GB\nCollected 40,000 docs, 0.181 GB\nCollected 60,000 docs, 0.271 GB\nCollected 80,000 docs, 0.361 GB\nCollected 100,000 docs, 0.451 GB\nCollected 120,000 docs, 0.542 GB\nCollected 140,000 docs, 0.632 GB\nCollected 160,000 docs, 0.722 GB\nCollected 180,000 docs, 0.811 GB\nCollected 200,000 docs, 0.902 GB\nCollected 220,000 docs, 0.991 GB\nSelected 238,301 documents, approx 1.074 GB\nSaved raw text file: /kaggle/working/hindi-gpt/nisram_hi_raw.txt size GB: 1.073857009\nTraining ByteLevel BPE tokenizer (vocab_size = 32000) ...\n\n\n\nTokenizer model files saved to: /kaggle/working/hindi-gpt/tokenizer\nTokenizer vocab size reported: 32000\nTokenizing texts and building token stream ...\nTokenized up to doc 500, tokens so far: 580,802\nTokenized up to doc 10,500, tokens so far: 11,988,665\nTokenized up to doc 20,500, tokens so far: 23,357,119\nTokenized up to doc 30,500, tokens so far: 34,779,078\nTokenized up to doc 40,500, tokens so far: 46,118,471\nTokenized up to doc 50,500, tokens so far: 57,335,192\nTokenized up to doc 60,500, tokens so far: 68,695,945\nTokenized up to doc 70,500, tokens so far: 80,066,706\nTokenized up to doc 80,500, tokens so far: 91,339,023\nTokenized up to doc 90,500, tokens so far: 102,694,022\nTokenized up to doc 100,500, tokens so far: 114,028,601\nTokenized up to doc 110,500, tokens so far: 125,344,266\nTokenized up to doc 120,500, tokens so far: 136,747,126\nTokenized up to doc 130,500, tokens so far: 148,175,479\nTokenized up to doc 140,500, tokens so far: 159,516,951\nTokenized up to doc 230,500, tokens so far: 261,127,117\nTotal tokens in dataset: 269922676\nTrain tokens: 242930408 Val tokens: 26992268\nModel parameters: 58.07M\nWrapped model in DataParallel for 2 GPUs\nStarting training for 50000 steps ...\nInitial eval — Train 10.5532, Val 10.5522\n[Step 2000] iter_loss 2.5141 | Eval Train 2.5418 Val 2.5685 | ValPPL 13.05 | eval_time 20.2s\nSaved checkpoint step 2000 (save_time 0.46s)\n[Step 4000] iter_loss 2.2342 | Eval Train 2.3284 Val 2.3401 | ValPPL 10.38 | eval_time 20.0s\nSaved checkpoint step 4000 (save_time 0.45s)\n[Step 6000] iter_loss 2.1841 | Eval Train 2.2284 Val 2.2109 | ValPPL 9.12 | eval_time 20.5s\nSaved checkpoint step 6000 (save_time 0.46s)\n[Step 8000] iter_loss 2.1545 | Eval Train 2.1458 Val 2.1414 | ValPPL 8.51 | eval_time 20.2s\nSaved checkpoint step 8000 (save_time 0.45s)\n[Step 10000] iter_loss 2.1062 | Eval Train 2.0914 Val 2.0920 | ValPPL 8.10 | eval_time 20.0s\nSaved checkpoint step 10000 (save_time 0.46s)\n[Step 12000] iter_loss 2.0700 | Eval Train 2.0585 Val 2.0710 | ValPPL 7.93 | eval_time 22.6s\nSaved checkpoint step 12000 (save_time 0.45s)\n[Step 14000] iter_loss 1.9747 | Eval Train 2.0058 Val 2.0253 | ValPPL 7.58 | eval_time 20.0s\nSaved checkpoint step 14000 (save_time 0.45s)\n[Step 16000] iter_loss 2.1857 | Eval Train 1.9936 Val 2.0002 | ValPPL 7.39 | eval_time 17.6s\nSaved checkpoint step 16000 (save_time 0.45s)\n[Step 18000] iter_loss 2.1133 | Eval Train 1.9672 Val 1.9552 | ValPPL 7.07 | eval_time 20.1s\nSaved checkpoint step 18000 (save_time 0.46s)\n[Step 20000] iter_loss 2.0046 | Eval Train 1.9423 Val 1.9755 | ValPPL 7.21 | eval_time 22.5s\nSaved checkpoint step 20000 (save_time 0.46s)\n[Step 22000] iter_loss 1.9127 | Eval Train 1.9224 Val 1.9356 | ValPPL 6.93 | eval_time 20.4s\nSaved checkpoint step 22000 (save_time 0.48s)\n[Step 24000] iter_loss 1.9959 | Eval Train 1.9281 Val 1.9207 | ValPPL 6.83 | eval_time 17.7s\nSaved checkpoint step 24000 (save_time 0.45s)\n[Step 26000] iter_loss 2.3786 | Eval Train 1.8933 Val 1.9003 | ValPPL 6.69 | eval_time 20.0s\nSaved checkpoint step 26000 (save_time 0.46s)\n[Step 28000] iter_loss 2.0800 | Eval Train 1.8781 Val 1.8907 | ValPPL 6.62 | eval_time 22.4s\nSaved checkpoint step 28000 (save_time 0.47s)\n[Step 30000] iter_loss 1.8277 | Eval Train 1.8955 Val 1.8773 | ValPPL 6.54 | eval_time 19.8s\nSaved checkpoint step 30000 (save_time 0.45s)\n[Step 32000] iter_loss 1.7899 | Eval Train 1.8445 Val 1.8376 | ValPPL 6.28 | eval_time 19.9s\nSaved checkpoint step 32000 (save_time 0.45s)\n[Step 34000] iter_loss 1.9059 | Eval Train 1.8619 Val 1.8818 | ValPPL 6.57 | eval_time 20.7s\nSaved checkpoint step 34000 (save_time 0.46s)\n[Step 36000] iter_loss 2.0034 | Eval Train 1.8461 Val 1.8647 | ValPPL 6.45 | eval_time 20.5s\nSaved checkpoint step 36000 (save_time 0.45s)\n[Step 38000] iter_loss 1.9307 | Eval Train 1.8490 Val 1.8680 | ValPPL 6.48 | eval_time 20.6s\nSaved checkpoint step 38000 (save_time 0.46s)\n[Step 40000] iter_loss 1.8547 | Eval Train 1.8607 Val 1.8623 | ValPPL 6.44 | eval_time 20.2s\nSaved checkpoint step 40000 (save_time 0.46s)\n[Step 42000] iter_loss 2.0384 | Eval Train 1.8288 Val 1.8322 | ValPPL 6.25 | eval_time 20.9s\nSaved checkpoint step 42000 (save_time 0.47s)\n[Step 44000] iter_loss 1.9370 | Eval Train 1.8211 Val 1.8123 | ValPPL 6.12 | eval_time 21.1s\nSaved checkpoint step 44000 (save_time 0.47s)\n[Step 46000] iter_loss 2.0757 | Eval Train 1.8278 Val 1.8595 | ValPPL 6.42 | eval_time 21.6s\nSaved checkpoint step 46000 (save_time 0.49s)\n[Step 48000] iter_loss 1.8044 | Eval Train 1.8152 Val 1.8137 | ValPPL 6.13 | eval_time 21.3s\nSaved checkpoint step 48000 (save_time 0.47s)\n[Step 50000] iter_loss 2.0117 | Eval Train 1.7750 Val 1.7953 | ValPPL 6.02 | eval_time 21.5s\nSaved checkpoint step 50000 (save_time 0.48s)\nTraining finished. Total time (s): 11869.327867031097\nFinal model saved (s): 0.49390411376953125\nGenerating sample for prompt (timing)...\nInference time (s) for 200 tokens: 9.835s\nSaved loss plot: /kaggle/working/hindi-gpt/loss_curve.png\nSaved perplexity plot: /kaggle/working/hindi-gpt/perplexity_curve.png\nSaved timings CSV: /kaggle/working/hindi-gpt/timings.csv\nSUMMARY:\n  total_steps: 50000\n  batch_size: 16\n  block_size: 128\n  d_model: 512\n  n_head: 8\n  n_layer: 8\n  vocab_size: 32000\n  final_checkpoint: /kaggle/working/hindi-gpt/checkpoints/gpt2_hindi_final.pt\n  tokenizer_dir: /kaggle/working/hindi-gpt/tokenizer\n  loss_plot: /kaggle/working/hindi-gpt/loss_curve.png\n  perplexity_plot: /kaggle/working/hindi-gpt/perplexity_curve.png\n  timings_csv: /kaggle/working/hindi-gpt/timings.csv\n  inference_time_s: 9.835065603256226\nAll outputs saved under: /kaggle/working/hindi-gpt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}